{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "976e77a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports and setup complete!\n",
      "‚úÖ YOLOXVideoPredictor class defined!\n",
      "‚úÖ Main inference function defined!\n",
      "‚úÖ Quick test function ready!\n",
      "\n",
      "üöÄ Ready to use! Call run_yolox_video_inference() with your video path\n",
      "üìù Example usage:\n",
      "run_yolox_video_inference(r\"C:\\path\\to\\your\\video.mp4\")\n",
      "üìù Or use quick_test() for interactive setup\n",
      "üé¨ Starting video inference on: C:\\Users\\Tairin Pairor\\Downloads\\NTUC-P-57_202506031631_202506031634.webm\n",
      "ü§ñ Using model: C:\\Users\\Tairin Pairor\\Documents\\Github\\Tarin%20Project\\person-detection\\models\\best_ckpt 1.pth\n",
      "‚öôÔ∏è Settings: conf=0.82, nms=0.45, size=416\n",
      "üîÑ Loading YOLOX model: yolox-nano\n",
      "üìä Model Summary: Params: 2.24M, Gflops: 2.93\n",
      "üì¶ Loading checkpoint from: C:\\Users\\Tairin Pairor\\Documents\\Github\\Tarin%20Project\\person-detection\\models\\best_ckpt 1.pth\n",
      "‚úÖ Model loaded successfully!\n",
      "üìπ Video info: 384x288, 5.0fps, 849 frames\n",
      "üéÆ Controls: 'q' to quit, 'p' to pause, SPACE to step frame\n",
      "Processing box 0: [  1.0801743 167.39421    96.99274   262.32913  ], score: 0.833284318447113, class_id: 0.0\n",
      "Processing box 0: [ 18.815279 152.20403  110.43065  258.26117 ], score: 0.8615871071815491, class_id: 0.0\n",
      "Processing box 0: [ 41.584805 151.21184  132.85     255.68    ], score: 0.8444907069206238, class_id: 0.0\n",
      "Processing box 0: [ 70.34908 158.98587 182.01418 228.08229], score: 0.8241068124771118, class_id: 0.0\n",
      "Processing box 0: [ 71.08099 158.1394  180.7138  231.96954], score: 0.8296497464179993, class_id: 0.0\n",
      "Processing box 0: [ 72.21415 155.44862 179.38498 229.34967], score: 0.8279728293418884, class_id: 0.0\n",
      "Processing box 0: [ 71.64257 152.8932  175.87134 229.19713], score: 0.8304137587547302, class_id: 0.0\n",
      "Processing box 0: [ 68.1746  145.90367 165.65213 228.26958], score: 0.8317030668258667, class_id: 0.0\n",
      "Processing box 0: [ 65.67354 141.00392 146.52507 232.06804], score: 0.8618450164794922, class_id: 0.0\n",
      "Processing box 0: [ 44.944885 137.56346  125.56999  237.38277 ], score: 0.8620914220809937, class_id: 0.0\n",
      "Processing box 0: [ 19.007677 143.65225  106.955444 232.67764 ], score: 0.8565651178359985, class_id: 0.0\n",
      "Processing box 0: [  4.886951 152.4754   110.341606 234.81883 ], score: 0.8438951969146729, class_id: 0.0\n",
      "Processing box 0: [ -5.446458 164.96565  103.81746  248.84187 ], score: 0.8204693794250488, class_id: 0.0\n",
      "üõë Stopping video inference\n",
      "‚úÖ Video inference completed!\n"
     ]
    }
   ],
   "source": [
    "# Jupyter Notebook Cell 1: Imports and Setup\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# YOLOX imports\n",
    "from yolox.data.data_augment import ValTransform\n",
    "from yolox.data.datasets import COCO_CLASSES\n",
    "from yolox.exp import get_exp\n",
    "from yolox.utils import fuse_model, get_model_info, postprocess\n",
    "\n",
    "# Custom visualization function\n",
    "def vis(img, boxes, scores, cls_ids, conf=0.5, class_names=None):\n",
    "    \"\"\"Enhanced visualization function with debug info\"\"\"\n",
    "    for i in range(len(boxes)):\n",
    "        print(f\"Processing box {i}: {boxes[i]}, score: {scores[i]}, class_id: {cls_ids[i]}\")\n",
    "        box = boxes[i]\n",
    "        cls_id = int(cls_ids[i])\n",
    "        score = scores[i]\n",
    "        if score < conf:\n",
    "            continue\n",
    "        x0 = int(box[0])\n",
    "        y0 = int(box[1])\n",
    "        x1 = int(box[2])\n",
    "        y1 = int(box[3])\n",
    "\n",
    "        color = (_COLORS[cls_id] * 255).astype(np.uint8).tolist()\n",
    "        text = '{}:{:.1f}%'.format(class_names[cls_id], score * 100)\n",
    "        txt_color = (0, 0, 0) if np.mean(_COLORS[cls_id]) > 0.5 else (255, 255, 255)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "        txt_size = cv2.getTextSize(text, font, 0.4, 1)[0]\n",
    "        cv2.rectangle(img, (x0, y0), (x1, y1), color, 2)\n",
    "\n",
    "        txt_bk_color = (_COLORS[cls_id] * 255 * 0.7).astype(np.uint8).tolist()\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (x0, y0 + 1),\n",
    "            (x0 + txt_size[0] + 1, y0 + int(1.5*txt_size[1])),\n",
    "            txt_bk_color,\n",
    "            -1\n",
    "        )\n",
    "        cv2.putText(img, text, (x0, y0 + txt_size[1]), font, 0.4, txt_color, thickness=1)\n",
    "\n",
    "    return img\n",
    "\n",
    "# Color palette for visualization\n",
    "_COLORS = np.array([\n",
    "    0.000, 0.447, 0.741,\n",
    "    0.850, 0.325, 0.098,\n",
    "    0.929, 0.694, 0.125,\n",
    "    0.494, 0.184, 0.556,\n",
    "    0.466, 0.674, 0.188,\n",
    "    0.301, 0.745, 0.933,\n",
    "    0.635, 0.078, 0.184,\n",
    "    0.300, 0.300, 0.300,\n",
    "    0.600, 0.600, 0.600,\n",
    "    1.000, 0.000, 0.000,\n",
    "    1.000, 0.500, 0.000,\n",
    "    0.749, 0.749, 0.000,\n",
    "    0.000, 1.000, 0.000,\n",
    "    0.000, 0.000, 1.000,\n",
    "    0.667, 0.000, 1.000,\n",
    "    0.333, 0.333, 0.000,\n",
    "    0.333, 0.667, 0.000,\n",
    "    0.333, 1.000, 0.000,\n",
    "    0.667, 0.333, 0.000,\n",
    "    0.667, 0.667, 0.000,\n",
    "    0.667, 1.000, 0.000,\n",
    "    1.000, 0.333, 0.000,\n",
    "    1.000, 0.667, 0.000,\n",
    "    1.000, 1.000, 0.000,\n",
    "    0.000, 0.333, 0.500,\n",
    "    0.000, 0.667, 0.500,\n",
    "    0.000, 1.000, 0.500,\n",
    "    0.333, 0.000, 0.500,\n",
    "    0.333, 0.333, 0.500,\n",
    "    0.333, 0.667, 0.500,\n",
    "    0.333, 1.000, 0.500,\n",
    "    0.667, 0.000, 0.500,\n",
    "    0.667, 0.333, 0.500,\n",
    "    0.667, 0.667, 0.500,\n",
    "    0.667, 1.000, 0.500,\n",
    "    1.000, 0.000, 0.500,\n",
    "    1.000, 0.333, 0.500,\n",
    "    1.000, 0.667, 0.500,\n",
    "    1.000, 1.000, 0.500,\n",
    "    0.000, 0.333, 1.000,\n",
    "    0.000, 0.667, 1.000,\n",
    "    0.000, 1.000, 1.000,\n",
    "    0.333, 0.000, 1.000,\n",
    "    0.333, 0.333, 1.000,\n",
    "    0.333, 0.667, 1.000,\n",
    "    0.333, 1.000, 1.000,\n",
    "    0.667, 0.000, 1.000,\n",
    "    0.667, 0.333, 1.000,\n",
    "    0.667, 0.667, 1.000,\n",
    "    0.667, 1.000, 1.000,\n",
    "    1.000, 0.000, 1.000,\n",
    "    1.000, 0.333, 1.000,\n",
    "    1.000, 0.667, 1.000,\n",
    "    0.333, 0.000, 0.000,\n",
    "    0.500, 0.000, 0.000,\n",
    "    0.667, 0.000, 0.000,\n",
    "    0.833, 0.000, 0.000,\n",
    "    1.000, 0.000, 0.000,\n",
    "    0.000, 0.167, 0.000,\n",
    "    0.000, 0.333, 0.000,\n",
    "    0.000, 0.500, 0.000,\n",
    "    0.000, 0.667, 0.000,\n",
    "    0.000, 0.833, 0.000,\n",
    "    0.000, 1.000, 0.000,\n",
    "    0.000, 0.000, 0.167,\n",
    "    0.000, 0.000, 0.333,\n",
    "    0.000, 0.000, 0.500,\n",
    "    0.000, 0.000, 0.667,\n",
    "    0.000, 0.000, 0.833,\n",
    "    0.000, 0.000, 1.000,\n",
    "    0.000, 0.000, 0.000,\n",
    "    0.143, 0.143, 0.143,\n",
    "    0.286, 0.286, 0.286,\n",
    "    0.429, 0.429, 0.429,\n",
    "    0.571, 0.571, 0.571,\n",
    "    0.714, 0.714, 0.714,\n",
    "    0.857, 0.857, 0.857,\n",
    "    0.000, 0.447, 0.741,\n",
    "    0.314, 0.717, 0.741,\n",
    "    0.50, 0.5, 0\n",
    "]).astype(np.float32).reshape(-1, 3)\n",
    "\n",
    "print(\"‚úÖ Imports and setup complete!\")\n",
    "\n",
    "# Jupyter Notebook Cell 2: YOLOX Predictor Class\n",
    "class YOLOXVideoPredictor:\n",
    "    def __init__(self, model_path, model_name=\"yolox-nano\", conf_threshold=0.3, nms_threshold=0.45, input_size=416):\n",
    "        \"\"\"\n",
    "        Initialize YOLOX predictor for video inference\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to the trained model checkpoint\n",
    "            model_name (str): YOLOX model variant (yolox-nano, yolox-s, etc.)\n",
    "            conf_threshold (float): Confidence threshold for detections\n",
    "            nms_threshold (float): NMS threshold\n",
    "            input_size (int): Input image size\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.nms_threshold = nms_threshold\n",
    "        self.input_size = (input_size, input_size)\n",
    "        \n",
    "        # Load experiment and model\n",
    "        print(f\"üîÑ Loading YOLOX model: {model_name}\")\n",
    "        self.exp = get_exp(exp_file=None, exp_name=model_name)\n",
    "        \n",
    "        # Override settings to match your trained model\n",
    "        self.exp.num_classes = 1  # Single class (person)\n",
    "        self.exp.class_names = [\"person\"]\n",
    "        self.exp.test_size = self.input_size\n",
    "        self.exp.test_conf = conf_threshold\n",
    "        self.exp.nmsthre = nms_threshold\n",
    "        \n",
    "        # Create model\n",
    "        self.model = self.exp.get_model()\n",
    "        print(f\"üìä Model Summary: {get_model_info(self.model, self.exp.test_size)}\")\n",
    "        \n",
    "        # Load checkpoint\n",
    "        print(f\"üì¶ Loading checkpoint from: {model_path}\")\n",
    "        if not Path(model_path).exists():\n",
    "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "        \n",
    "        ckpt = torch.load(model_path, map_location=\"cpu\", weights_only=False)\n",
    "        self.model.load_state_dict(ckpt[\"model\"], strict=False)  # Use strict=False for compatibility\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Setup preprocessing\n",
    "        self.preproc = ValTransform(legacy=False)\n",
    "        \n",
    "        print(\"‚úÖ Model loaded successfully!\")\n",
    "    \n",
    "    def inference(self, frame):\n",
    "        \"\"\"Run inference on a single frame\"\"\"\n",
    "        height, width = frame.shape[:2]\n",
    "        \n",
    "        # Calculate resize ratio\n",
    "        ratio = min(self.input_size[0] / height, self.input_size[1] / width)\n",
    "        \n",
    "        # Preprocess frame\n",
    "        img, _ = self.preproc(frame, None, self.input_size)\n",
    "        img = torch.from_numpy(img).unsqueeze(0).float()\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(img)\n",
    "            outputs = postprocess(\n",
    "                outputs, \n",
    "                self.exp.num_classes, \n",
    "                self.conf_threshold,\n",
    "                self.nms_threshold, \n",
    "                class_agnostic=True\n",
    "            )\n",
    "        \n",
    "        return outputs[0], ratio\n",
    "    \n",
    "    def visualize(self, frame, detections, ratio):\n",
    "        \"\"\"Visualize detections on frame\"\"\"\n",
    "        if detections is None:\n",
    "            return frame\n",
    "        \n",
    "        # Convert detections to numpy\n",
    "        detections = detections.cpu().numpy()\n",
    "        \n",
    "        # Extract components\n",
    "        bboxes = detections[:, 0:4] / ratio  # Scale back to original size\n",
    "        scores = detections[:, 4] * detections[:, 5]  # obj_conf * cls_conf\n",
    "        cls_ids = detections[:, 6]\n",
    "        \n",
    "        # Use visualization function\n",
    "        result_frame = vis(\n",
    "            frame.copy(), \n",
    "            bboxes, \n",
    "            scores, \n",
    "            cls_ids, \n",
    "            conf=self.conf_threshold, \n",
    "            class_names=self.exp.class_names\n",
    "        )\n",
    "        \n",
    "        return result_frame\n",
    "\n",
    "print(\"‚úÖ YOLOXVideoPredictor class defined!\")\n",
    "\n",
    "# Jupyter Notebook Cell 3: Main Video Inference Function\n",
    "def run_yolox_video_inference(\n",
    "    video_path, \n",
    "    model_path=r\"C:\\Users\\Tairin Pairor\\Documents\\Github\\Tarin%20Project\\person-detection\\models\\best_ckpt 1.pth\",\n",
    "    conf_threshold=0.3,\n",
    "    nms_threshold=0.45,\n",
    "    input_size=416,\n",
    "    model_name=\"yolox-nano\",\n",
    "    show_fps=True,\n",
    "    flip_video=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Run YOLOX inference on video with live display\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to input video file\n",
    "        model_path (str): Path to YOLOX model checkpoint\n",
    "        conf_threshold (float): Confidence threshold for detections\n",
    "        nms_threshold (float): NMS threshold  \n",
    "        input_size (int): Model input size\n",
    "        model_name (str): YOLOX model variant\n",
    "        show_fps (bool): Display FPS counter\n",
    "        flip_video (bool): Flip video vertically (for thermal cameras)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üé¨ Starting video inference on: {video_path}\")\n",
    "    print(f\"ü§ñ Using model: {model_path}\")\n",
    "    print(f\"‚öôÔ∏è Settings: conf={conf_threshold}, nms={nms_threshold}, size={input_size}\")\n",
    "    \n",
    "    # Initialize predictor\n",
    "    try:\n",
    "        predictor = YOLOXVideoPredictor(\n",
    "            model_path=model_path,\n",
    "            model_name=model_name,\n",
    "            conf_threshold=conf_threshold,\n",
    "            nms_threshold=nms_threshold,\n",
    "            input_size=input_size\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize predictor: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"‚ùå Failed to open video: {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    print(f\"üìπ Video info: {width}x{height}, {fps:.1f}fps, {frame_count} frames\")\n",
    "    print(\"üéÆ Controls: 'q' to quit, 'p' to pause, SPACE to step frame\")\n",
    "    \n",
    "    # Create window\n",
    "    cv2.namedWindow(\"YOLOX Person Detection\", cv2.WINDOW_NORMAL)\n",
    "    # cv2.resizeWindow(\"YOLOX Person Detection\", 1024, 768)\n",
    "    \n",
    "    frame_idx = 0\n",
    "    paused = False\n",
    "    last_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        if not paused:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"üì∫ End of video reached\")\n",
    "                break\n",
    "            \n",
    "            frame_idx += 1\n",
    "        \n",
    "        # Optional: flip frame (useful for thermal cameras)\n",
    "        if flip_video:\n",
    "            frame = cv2.flip(frame, 0)\n",
    "        \n",
    "        # Run inference\n",
    "        start_time = time.time()\n",
    "        detections, ratio = predictor.inference(frame)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Visualize results\n",
    "        result_frame = predictor.visualize(frame, detections, ratio)\n",
    "        \n",
    "        # Add info overlay\n",
    "        info_text = []\n",
    "        info_text.append(f\"Frame: {frame_idx}/{frame_count}\")\n",
    "        info_text.append(f\"Inference: {inference_time*1000:.1f}ms\")\n",
    "        \n",
    "        if detections is not None:\n",
    "            person_count = len(detections)\n",
    "            info_text.append(f\"Persons: {person_count}\")\n",
    "        else:\n",
    "            info_text.append(\"Persons: 0\")\n",
    "        \n",
    "        if show_fps:\n",
    "            current_time = time.time()\n",
    "            display_fps = 1.0 / (current_time - last_time) if (current_time - last_time) > 0 else 0\n",
    "            info_text.append(f\"FPS: {display_fps:.1f}\")\n",
    "            last_time = current_time\n",
    "        \n",
    "        # Draw info overlay\n",
    "        # y_offset = 30\n",
    "        # for text in info_text:\n",
    "        #     cv2.putText(result_frame, text, (10, y_offset), \n",
    "        #                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        #     y_offset += 30\n",
    "        \n",
    "        # Add status bar\n",
    "        if paused:\n",
    "            cv2.putText(result_frame, \"PAUSED - Press 'p' to resume\", \n",
    "                       (10, height - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "        \n",
    "        # Display frame\n",
    "        cv2.imshow(\"YOLOX Person Detection\", result_frame)\n",
    "        \n",
    "        # Handle keyboard input\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or key == 27:  # 'q' or ESC\n",
    "            print(\"üõë Stopping video inference\")\n",
    "            break\n",
    "        elif key == ord('p'):  # Toggle pause\n",
    "            paused = not paused\n",
    "            print(f\"‚è∏Ô∏è Paused: {paused}\")\n",
    "        elif key == ord(' ') and paused:  # Step frame when paused\n",
    "            paused = False\n",
    "            continue\n",
    "        elif key == ord('r'):  # Restart video\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            frame_idx = 0\n",
    "            print(\"üîÑ Restarting video\")\n",
    "    \n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"‚úÖ Video inference completed!\")\n",
    "\n",
    "print(\"‚úÖ Main inference function defined!\")\n",
    "\n",
    "# Jupyter Notebook Cell 4: Quick Test Function\n",
    "def quick_test():\n",
    "    \"\"\"Quick test with default settings\"\"\"\n",
    "    video_path = input(\"Enter video path (or press Enter for webcam): \").strip()\n",
    "    \n",
    "    if not video_path:\n",
    "        print(\"üìπ Using webcam (camera 0)\")\n",
    "        video_path = 0\n",
    "    \n",
    "    run_yolox_video_inference(\n",
    "        video_path=video_path,\n",
    "        conf_threshold=0.3,\n",
    "        nms_threshold=0.45,\n",
    "        show_fps=True,\n",
    "        flip_video=True  # Set to True for thermal cameras\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Quick test function ready!\")\n",
    "print(\"\\nüöÄ Ready to use! Call run_yolox_video_inference() with your video path\")\n",
    "print(\"üìù Example usage:\")\n",
    "print('run_yolox_video_inference(r\"C:\\\\path\\\\to\\\\your\\\\video.mp4\")')\n",
    "print(\"üìù Or use quick_test() for interactive setup\")\n",
    "\n",
    "# path =  r\"C:\\Users\\Tairin Pairor\\Downloads\\NTUC-P-57_202506031442_202506031445.webm\"\n",
    "path = r\"C:\\Users\\Tairin Pairor\\Downloads\\NTUC-P-57_202506031631_202506031634.webm\"\n",
    "# path = r\"C:\\Users\\Tairin Pairor\\Downloads\\ruoxuan.webm\"\n",
    "# path = r\"C:\\Users\\Tairin Pairor\\Downloads\\NTUC-P-57_202506031838_202506031840.webm\"\n",
    "# path = r\"C:\\Users\\Tairin Pairor\\Downloads\\NTUC-P-57_202506041801_202506041803.webm\"\n",
    "\n",
    "# Example usage for your specific case:\n",
    "run_yolox_video_inference(\n",
    "    video_path=path,\n",
    "    conf_threshold=0.82,\n",
    "    nms_threshold=0.45,\n",
    "    flip_video=False  # For thermal camera footage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b59ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
